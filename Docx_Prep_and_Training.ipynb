{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook shows how docx files are converted to html files, how text and features from html files are joined to their labels files, how the dataset is prepped for training, some training attempts and the final models and scalers currently used for the auto-peer project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cssutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-05ee1499fbf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mExtracting_Features_html_new\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mE_F\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExtract_Labels\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mE_L\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mJoin_Features_Labels\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mJ_F_L\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\emarefa\\emarefa-auto_peer_review-bbef8314ab1a\\emarefa-auto_peer_review-bbef8314ab1a\\Extracting_Features_html_new.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcssutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcssutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetLevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCRITICAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cssutils'"
     ]
    }
   ],
   "source": [
    "import Extracting_Features_html_new as E_F\n",
    "import Extract_Labels as E_L\n",
    "import Join_Features_Labels as J_F_L\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import subprocess\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert doc files to docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (32).doc\n",
      "0 (25).doc\n",
      "0 (31).doc\n",
      "0 (26).doc\n",
      "0 (29).doc\n",
      "0 (30).doc\n",
      "0 (27).doc\n",
      "0 (13)tt.doc\n",
      "0 (21)tt.doc\n",
      "0 (34).doc\n",
      "0 (35).doc\n",
      "0 (3)tt.doc\n",
      "0 (11)tt.doc\n",
      "0 (12)tt.doc\n",
      "0 (24).doc\n",
      "0 (22)tt.doc\n",
      "0 (23)tt.doc\n",
      "0 (7)tt.doc\n",
      "0 (2)tt.doc\n",
      "0 (9)tt.doc\n",
      "0 (16)tt.doc\n",
      "0 (15)tt.doc\n",
      "0 (4)tt.doc\n",
      "0 (5)tt.doc\n",
      "0 (8)tt.doc\n",
      "0 (20)tt.doc\n",
      "0 (33).doc\n",
      "0 (6)tt.doc\n",
      "0 (10)tt.doc\n",
      "0 (19)tt.doc\n",
      "0 (14tt).doc\n",
      "0 (17)tt.doc\n",
      "0 (18)tt.doc\n",
      "0 (1)tt.doc\n",
      "0 (28).doc\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('test3/'):\n",
    "    if file.endswith('.doc'):\n",
    "        print(file)\n",
    "        fn = 'test3/'+file\n",
    "        try:\n",
    "            subprocess.call(['lowriter', '--headless', '--convert-to', 'docx', fn])\n",
    "        except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert docx files to html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting file\n",
      "converting file\n",
      "converting file\n",
      "converting file\n",
      "converting file\n",
      "converting file\n",
      "converting file\n",
      "converting file\n",
      "converting file\n",
      "converting file\n",
      "converting file\n",
      "converting file\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "for file in os.listdir('test4/'):\n",
    "    if file.endswith('.docx'):\n",
    "        fn = 'test4/'+file\n",
    "        try:\n",
    "            subprocess.call(['soffice', '--headless', '--convert-to', 'html:XHTML Writer File:UTF8', fn])\n",
    "            print('converting file')\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move html file to html folder\n",
    "for file in os.listdir():\n",
    "    if file.endswith('.html'):\n",
    "        os.replace(file, \"new_html_files/\"+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## match each html to its labels file\n",
    "dict_labels_to_html = collections.defaultdict(list)\n",
    "for html_file in os.listdir('new_html_files/'):\n",
    "    file_por = html_file.split('.html')[0]\n",
    "    label_file = file_por+'.txt.aif.xml'\n",
    "    if label_file in os.listdir('labels/'):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_labels_to_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5ff.txt.aif.xml\n",
      "['5ff.html']\n",
      "splitting emails\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/auto_peer_review/Extracting_Features_html_new.py:700: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  df['contains_main_keywords'] = np.where(df['level_0'].str.contains(r'(كلمات|الكلمات)\\s*(المفتاحيّة|المفتاحيّه|الداله|الاصطلاحية|المفتاحيه|المفاتيح|الدالّة|مفتاحية|مفتاحيّة|المفتاح|الافتتاحية|مفتاحيه|المفتاحية|الدالة)'),1,0)\n",
      "/home/yasmeen/auto_peer_review/Extracting_Features_html_new.py:770: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  df['contains_url'] = np.where(df['level_0'].str.contains(r'(https?://[^\\s]+)'), 1, 0)\n",
      "/home/yasmeen/auto_peer_review/Extracting_Features_html_new.py:847: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  df['contains_headings'] = np.where(df['level_0'].str.strip().str.lstrip('1234567890\\s?\\.?-?\\(?\\)?\\/?\\s?').str.strip(':|.| |-|/|–').str.strip().str.contains('^(المقدمة|مقدمة|تقدمة|المقدمة وأهمية البحث|Introduction|المقدمة وأهمية البحثIntroduction|تمهيد|مشكلة البحث|مشكلة الدراسة|إشكالية الدراسة|إشكالية البحث|أهمية البحث|أهمية الدراسة|أهداف الدراسة|أهداف البحث|هدف الدراسة|هدف البحث|هدفا البحث|هدفا الدراسة|فرضية البحث|فرضية الدراسة|فرضيات البحث|فرضيات الدراسة|التعريف بالبحث|التعريف بالدراسة|منهج البحث|منهج الدراسة|منهجية الدراسة|منهجية البحث|الإطار المنهجي|الإطار المنهجي للبحث|الإطار المنهجي للدراسة|حدود البحث|حدود الدراسة|هيكل الدراسة|هيكل البحث|نموذج الدراسة|نموذج البحث|أنموذج البحث|أنموذج الدراسة|تساؤلات الدراسة|تساؤلات البحث|أسئلة الدراسة|أسئلة البحث|موضوع الدراسة|موضوع البحث|الإطار الزمني|الإطار الزمني للبحث|الإطار الزمني للدراسة|الإطار النظري|الحدود الزمنية|مجتمع الدراسة|مجتمع البحث|عينة الدراسة|عينة البحث|أداة البحث|أداة الدراسة|طريقة جمع العينة|طرق جمع العينات|نتائج الدراسة ومناقشتها|الدراسات السابقة|أساليب جمع البيانات|النتائج|نتائج الدراسة|نتائج البحث|النتائج ومناقشتها|الاستنتاجات|المقترحات|الاستنتاجات والمقترحات|التوصيات|التوصيات والمقترحات|الخاتمة|خاتمة)\\s*[:\\n-]?\\s*'),1,0)\n",
      "/home/yasmeen/auto_peer_review/Extracting_Features_html_new.py:849: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  df['contains_reference_heading'] = np.where(df['level_0'].str.strip().str.lstrip('1234567890\\s?\\.?-?\\(?\\)?\\/?\\s?').str.strip(':|.| |-|/|–').str.strip().str.contains('^(المصادر|الإحالات والمراجع|المراجع|الهوامش|الهوامش و\\s?المراجع|المراجع و\\s?الهوامش|الهوامش والمراجع المعتمدة|المصادر والمراجع|ثبت المصادر والمراجع|المراجع والاحالات|الهوامش والمصادر|هوامش البحث|المصادر والمراجع المعتمدة|هوامش وتعليقات البحث|قائمة المصادر والمراجع|قائمة المراجع|مراجع وهوامش البحث|الحواشي والتعليقات|قائمة المصادر|المراجع العربية|المراجع الأجنبية|المراجع الإنجليزية|مراجع وهوامش البحث|مكتبة البحث|الاحالات والمراجع|References|المصادر References|هوامش الدراسة|الإحالات|المصادر والإحالات المعتمدة|المراجع باللّغة العربية|المراجع باللّغة الإنجليزية|مراجع باللغة العربية|مراجع باللغة الإنجليزية|الببليوغرافيا|المراجع References)\\s*[:\\n-]?\\s*'),1,0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bab5.txt.aif.xml\n",
      "['bab5.html']\n",
      "splitting emails\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/auto_peer_review/Join_Features_Labels.py:111: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  row_to_add = self.features_df[self.features_df['labels'].isnull()][self.features_df['level_0'] == matched[0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8073-14647-1-PB.txt.aif.xml\n",
      "['8073-14647-1-PB.html']\n",
      "Roa Iktissadia17-15.txt.aif.xml\n",
      "['Roa Iktissadia17-15.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "رامي حلاوة + تيسير المنسي+عبدالسلام جابر   (8).txt.aif.xml\n",
      "['رامي حلاوة + تيسير المنسي+عبدالسلام جابر   (8).html']\n",
      "2.txt.aif.xml\n",
      "['2.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "8071-14648-1-PB.txt.aif.xml\n",
      "['8071-14648-1-PB.html']\n",
      "0 (7).txt.aif.xml\n",
      "['0 (7).html']\n",
      "9125617.txt.aif.xml\n",
      "['9125617.html']\n",
      "splitting emails\n",
      "0 (1)aaa.txt.aif.xml\n",
      "['0 (1)aaa.html']\n",
      "Roa Iktissadia17-4.txt.aif.xml\n",
      "['Roa Iktissadia17-4.html']\n",
      "splitting emails\n",
      "أكرم الحجايا    (5)   مشترك.txt.aif.xml\n",
      "['أكرم الحجايا    (5)   مشترك.html']\n",
      "علي أبو سليم   (8).txt.aif.xml\n",
      "['علي أبو سليم   (8).html']\n",
      "0 (6).txt.aif.xml\n",
      "['0 (6).html']\n",
      "(8)شاهر أبو شريخ اثر استخدام استراتيجيات التفكير.txt.aif.xml\n",
      "['(8)شاهر أبو شريخ اثر استخدام استراتيجيات التفكير.html']\n",
      "8043-14609-1-PB.txt.aif.xml\n",
      "['8043-14609-1-PB.html']\n",
      "نماء محمد البنا    (1).txt.aif.xml\n",
      "['نماء محمد البنا    (1).html']\n",
      "11964.txt.aif.xml\n",
      "['11964.html']\n",
      "23-دراسة بعض الجوانب الفسيولوجية والتركيب المعدني.txt.aif.xml\n",
      "['23-دراسة بعض الجوانب الفسيولوجية والتركيب المعدني.html']\n",
      "15.txt.aif.xml\n",
      "['15.html']\n",
      "6ff.txt.aif.xml\n",
      "['6ff.html']\n",
      "splitting emails\n",
      "8057-14631-1-PB.txt.aif.xml\n",
      "['8057-14631-1-PB.html']\n",
      "8078-14651-1-PB.txt.aif.xml\n",
      "['8078-14651-1-PB.html']\n",
      "05.txt.aif.xml\n",
      "['05.html']\n",
      "0 (2)abcd.txt.aif.xml\n",
      "['0 (2)abcd.html']\n",
      "ROA 15-6.txt.aif.xml\n",
      "['ROA 15-6.html']\n",
      "1 (3).txt.aif.xml\n",
      "['1 (3).html']\n",
      "0 (1).txt.aif.xml\n",
      "['0 (1).html']\n",
      "8080-14656-1-PB.txt.aif.xml\n",
      "['8080-14656-1-PB.html']\n",
      "5943.txt.aif.xml\n",
      "['5943.html']\n",
      "1 (14).txt.aif.xml\n",
      "['1 (14).html']\n",
      "2fff.txt.aif.xml\n",
      "['2fff.html']\n",
      "tree.txt.aif.xml\n",
      "['tree.html']\n",
      "د عائشه بوكريسة.txt.aif.xml\n",
      "['د عائشه بوكريسة.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "ABPR_04_3_AR.txt.aif.xml\n",
      "['ABPR_04_3_AR.html']\n",
      "16sgs.txt.aif.xml\n",
      "['16sgs.html']\n",
      "1 (5).txt.aif.xml\n",
      "['1 (5).html']\n",
      "splitting emails\n",
      "1 (17).txt.aif.xml\n",
      "['1 (17).html']\n",
      "splitting emails\n",
      "1 a-ee.txt.aif.xml\n",
      "['1 a-ee.html']\n",
      "معاوية ابو غزال  (3).txt.aif.xml\n",
      "['معاوية ابو غزال  (3).html']\n",
      "Roa Iktissadia17-2.txt.aif.xml\n",
      "['Roa Iktissadia17-2.html']\n",
      "splitting emails\n",
      "3-a-e1.txt.aif.xml\n",
      "['3-a-e1.html']\n",
      "ROA 15-1.txt.aif.xml\n",
      "['ROA 15-1.html']\n",
      "fif.txt.aif.xml\n",
      "['fif.html']\n",
      "ABPR_04_5_AR.txt.aif.xml\n",
      "['ABPR_04_5_AR.html']\n",
      "2 a-e.txt.aif.xml\n",
      "['2 a-e.html']\n",
      "ليلى وصلاح.txt.aif.xml\n",
      "['ليلى وصلاح.html']\n",
      "21.txt.aif.xml\n",
      "['21.html']\n",
      "1ff.txt.aif.xml\n",
      "['1ff.html']\n",
      "ten.txt.aif.xml\n",
      "['ten.html']\n",
      "0 (3).txt.aif.xml\n",
      "['0 (3).html']\n",
      "1 (15).txt.aif.xml\n",
      "['1 (15).html']\n",
      "_R11-17 __.txt.aif.xml\n",
      "['_R11-17 __.html']\n",
      "20yffjk.txt.aif.xml\n",
      "['20yffjk.html']\n",
      "توفيق 26-50.txt.aif.xml\n",
      "['توفيق 26-50.html']\n",
      "1-14_عربى.txt.aif.xml\n",
      "['1-14_عربى.html']\n",
      "22.txt.aif.xml\n",
      "['22.html']\n",
      "splitting emails\n",
      "البحث الثالث عربي ذنون وامير.txt.aif.xml\n",
      "['البحث الثالث عربي ذنون وامير.html']\n",
      "Roa Iktissadia17-5.txt.aif.xml\n",
      "['Roa Iktissadia17-5.html']\n",
      "splitting emails\n",
      "د محمد  العبسي ، الفقية.txt.aif.xml\n",
      "['د محمد  العبسي ، الفقية.html']\n",
      "8086-14661-1-PB.txt.aif.xml\n",
      "['8086-14661-1-PB.html']\n",
      "ABPR_04_4_AR.txt.aif.xml\n",
      "['ABPR_04_4_AR.html']\n",
      "Roa Iktissadia17-3.txt.aif.xml\n",
      "['Roa Iktissadia17-3.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "0 (11)aa.txt.aif.xml\n",
      "['0 (11)aa.html']\n",
      "Roa Iktissadia17-21.txt.aif.xml\n",
      "['Roa Iktissadia17-21.html']\n",
      "1 (19).txt.aif.xml\n",
      "['1 (19).html']\n",
      "835807.txt.aif.xml\n",
      "['835807.html']\n",
      "2 a-ee.txt.aif.xml\n",
      "['2 a-ee.html']\n",
      "18.txt.aif.xml\n",
      "['18.html']\n",
      "splitting emails\n",
      "1-new.txt.aif.xml\n",
      "['1-new.html']\n",
      "1 (6).txt.aif.xml\n",
      "['1 (6).html']\n",
      "صادق شديفات  ( 9   ).txt.aif.xml\n",
      "['صادق شديفات  ( 9   ).html']\n",
      "(9)  هيثم عيادات.txt.aif.xml\n",
      "['(9)  هيثم عيادات.html']\n",
      "علاء عبد اللطيف 116-122.txt.aif.xml\n",
      "['علاء عبد اللطيف 116-122.html']\n",
      "6fff.txt.aif.xml\n",
      "['6fff.html']\n",
      "ABPR_04_7_AR.txt.aif.xml\n",
      "['ABPR_04_7_AR.html']\n",
      "jsport_paper_2015_112645461.txt.aif.xml\n",
      "['jsport_paper_2015_112645461.html']\n",
      "‫R11-3 ‫‬.txt.aif.xml\n",
      "['\\u202bR11-3 \\u202b\\u202c.html']\n",
      "Roa Iktissadia17-14.txt.aif.xml\n",
      "['Roa Iktissadia17-14.html']\n",
      "bab9.txt.aif.xml\n",
      "['bab9.html']\n",
      "splitting emails\n",
      "زين العابدين  بني هاني   (10).txt.aif.xml\n",
      "['زين العابدين  بني هاني   (10).html']\n",
      "2-new.txt.aif.xml\n",
      "['2-new.html']\n",
      "سالم و دلير.txt.aif.xml\n",
      "['سالم و دلير.html']\n",
      "1 (21).txt.aif.xml\n",
      "['1 (21).html']\n",
      "ROA 15-11.txt.aif.xml\n",
      "['ROA 15-11.html']\n",
      "1fff.txt.aif.xml\n",
      "['1fff.html']\n",
      "Roa Iktissadia17-19.txt.aif.xml\n",
      "['Roa Iktissadia17-19.html']\n",
      "splitting emails\n",
      "1205-2816-1-PB.txt.aif.xml\n",
      "['1205-2816-1-PB.html']\n",
      "8085-14660-1-PB.txt.aif.xml\n",
      "['8085-14660-1-PB.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "12.txt.aif.xml\n",
      "['12.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "31.txt.aif.xml\n",
      "['31.html']\n",
      "محمد العمرات (8) مسترد.txt.aif.xml\n",
      "['محمد العمرات (8) مسترد.html']\n",
      "1534 (1).txt.aif.xml\n",
      "['1534 (1).html']\n",
      "هيثم النادر  مشترك (4).txt.aif.xml\n",
      "['هيثم النادر  مشترك (4).html']\n",
      "11440655.txt.aif.xml\n",
      "['11440655.html']\n",
      "splitting emails\n",
      "امها ذنون - البحث التاسع.txt.aif.xml\n",
      "['امها ذنون - البحث التاسع.html']\n",
      "5957.txt.aif.xml\n",
      "['5957.html']\n",
      "1 (12).txt.aif.xml\n",
      "['1 (12).html']\n",
      "21fffff.txt.aif.xml\n",
      "['21fffff.html']\n",
      "سالم خوالده.txt.aif.xml\n",
      "['سالم خوالده.html']\n",
      "0 (8).txt.aif.xml\n",
      "['0 (8).html']\n",
      "امل نبيل بدر.txt.aif.xml\n",
      "['امل نبيل بدر.html']\n",
      "splitting emails\n",
      "string indices must be integers\n",
      "wa8.txt.aif.xml\n",
      "['wa8.html']\n",
      "359331.txt.aif.xml\n",
      "['359331.html']\n",
      "splitting emails\n",
      "8076-14652-1-PB.txt.aif.xml\n",
      "['8076-14652-1-PB.html']\n",
      "1 (4).txt.aif.xml\n",
      "['1 (4).html']\n",
      "oneeee.txt.aif.xml\n",
      "['oneeee.html']\n",
      "0 (6)aa.txt.aif.xml\n",
      "['0 (6)aa.html']\n",
      "bab10.txt.aif.xml\n",
      "['bab10.html']\n",
      "Roa Iktissadia17-7.txt.aif.xml\n",
      "['Roa Iktissadia17-7.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "0 (2)aa.txt.aif.xml\n",
      "['0 (2)aa.html']\n",
      "نبيل الشطناوي وزبن الزبن.txt.aif.xml\n",
      "['نبيل الشطناوي وزبن الزبن.html']\n",
      "1 a-e.txt.aif.xml\n",
      "['1 a-e.html']\n",
      "02-07-OK06.txt.aif.xml\n",
      "['02-07-OK06.html']\n",
      "15-a-e1.txt.aif.xml\n",
      "['15-a-e1.html']\n",
      "splitting emails\n",
      "‫R11-27.txt.aif.xml\n",
      "['\\u202bR11-27.html']\n",
      "splitting emails\n",
      "bab2.txt.aif.xml\n",
      "['bab2.html']\n",
      "elven.txt.aif.xml\n",
      "['elven.html']\n",
      "11.txt.aif.xml\n",
      "['11.html']\n",
      "splitting emails\n",
      "Roa Iktissadia17-17.txt.aif.xml\n",
      "['Roa Iktissadia17-17.html']\n",
      "الهام سعيد5.txt.aif.xml\n",
      "['الهام سعيد5.html']\n",
      "(17) نافر البقيعي.txt.aif.xml\n",
      "['(17) نافر البقيعي.html']\n",
      "ميسون الزعبي   (9).txt.aif.xml\n",
      "['ميسون الزعبي   (9).html']\n",
      "19dffsd.txt.aif.xml\n",
      "['19dffsd.html']\n",
      "1206-2819-1-PB.txt.aif.xml\n",
      "['1206-2819-1-PB.html']\n",
      "ROA 15-2.txt.aif.xml\n",
      "['ROA 15-2.html']\n",
      "splitting emails\n",
      "جواد كاظم 51-87.txt.aif.xml\n",
      "['جواد كاظم 51-87.html']\n",
      "1 (20).txt.aif.xml\n",
      "['1 (20).html']\n",
      "3-new.txt.aif.xml\n",
      "['3-new.html']\n",
      "2ff.txt.aif.xml\n",
      "['2ff.html']\n",
      "splitting emails\n",
      "bab6.txt.aif.xml\n",
      "['bab6.html']\n",
      "splitting emails\n",
      "wa6.txt.aif.xml\n",
      "['wa6.html']\n",
      "(8) ماهر زيادات.txt.aif.xml\n",
      "['(8) ماهر زيادات.html']\n",
      "25.txt.aif.xml\n",
      "['25.html']\n",
      "splitting emails\n",
      "string indices must be integers\n",
      "1 (10).txt.aif.xml\n",
      "['1 (10).html']\n",
      "11963.txt.aif.xml\n",
      "['11963.html']\n",
      "8039-14607-1-PB.txt.aif.xml\n",
      "['8039-14607-1-PB.html']\n",
      "1 (23).txt.aif.xml\n",
      "['1 (23).html']\n",
      "‫R11-26.txt.aif.xml\n",
      "['\\u202bR11-26.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "5.txt.aif.xml\n",
      "['5.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "1 (8).txt.aif.xml\n",
      "['1 (8).html']\n",
      "splitting emails\n",
      "عبد الرحمن.txt.aif.xml\n",
      "['عبد الرحمن.html']\n",
      "0 (1)abcd.txt.aif.xml\n",
      "['0 (1)abcd.html']\n",
      "01-سعدون.txt.aif.xml\n",
      "['01-سعدون.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "البجث الثاني.txt.aif.xml\n",
      "['البجث الثاني.html']\n",
      "Roa Iktissadia17-11.txt.aif.xml\n",
      "['Roa Iktissadia17-11.html']\n",
      "splitting emails\n",
      "بحث 12  (مدقق) حمزة المجدلاوي وعدنان العابد.txt.aif.xml\n",
      "['بحث 12  (مدقق) حمزة المجدلاوي وعدنان العابد.html']\n",
      "3 a-ee.txt.aif.xml\n",
      "['3 a-ee.html']\n",
      "wa1.txt.aif.xml\n",
      "['wa1.html']\n",
      "2 A.txt.aif.xml\n",
      "['2 A.html']\n",
      "13frw.txt.aif.xml\n",
      "['13frw.html']\n",
      "1-a-e1.txt.aif.xml\n",
      "['1-a-e1.html']\n",
      "bab4.txt.aif.xml\n",
      "['bab4.html']\n",
      "24.txt.aif.xml\n",
      "['24.html']\n",
      "splitting emails\n",
      "08.txt.aif.xml\n",
      "['08.html']\n",
      "1 (7).txt.aif.xml\n",
      "['1 (7).html']\n",
      "1 (16).txt.aif.xml\n",
      "['1 (16).html']\n",
      "01.txt.aif.xml\n",
      "['01.html']\n",
      "أحمد جاسم 1- 25.txt.aif.xml\n",
      "['أحمد جاسم 1- 25.html']\n",
      "0 (10).txt.aif.xml\n",
      "['0 (10).html']\n",
      "1 (9).txt.aif.xml\n",
      "['1 (9).html']\n",
      "splitting emails\n",
      "0 (11)abcd.txt.aif.xml\n",
      "['0 (11)abcd.html']\n",
      "0 (1)aa.txt.aif.xml\n",
      "['0 (1)aa.html']\n",
      "Roa Iktissadia17-18.txt.aif.xml\n",
      "['Roa Iktissadia17-18.html']\n",
      "5956.txt.aif.xml\n",
      "['5956.html']\n",
      "(12) أحمد العكور.txt.aif.xml\n",
      "['(12) أحمد العكور.html']\n",
      "11966.txt.aif.xml\n",
      "['11966.html']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABPR_04_6_AR.txt.aif.xml\n",
      "['ABPR_04_6_AR.html']\n",
      "‫‫R11-9.txt.aif.xml\n",
      "['\\u202b\\u202bR11-9.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "Roa Iktissadia17-13.txt.aif.xml\n",
      "['Roa Iktissadia17-13.html']\n",
      "bab8.txt.aif.xml\n",
      "['bab8.html']\n",
      "1.txt.aif.xml\n",
      "['1.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "16-a-e1.txt.aif.xml\n",
      "['16-a-e1.html']\n",
      "01-10-ok02.txt.aif.xml\n",
      "['01-10-ok02.html']\n",
      "8045-14625-1-PB.txt.aif.xml\n",
      "['8045-14625-1-PB.html']\n",
      "9aass.txt.aif.xml\n",
      "['9aass.html']\n",
      "splitting emails\n",
      "03.txt.aif.xml\n",
      "['03.html']\n",
      "04.txt.aif.xml\n",
      "['04.html']\n",
      "14-a-e1.txt.aif.xml\n",
      "['14-a-e1.html']\n",
      "splitting emails\n",
      "R11-25.txt.aif.xml\n",
      "['R11-25.html']\n",
      "عمران ملحم   (8).txt.aif.xml\n",
      "['عمران ملحم   (8).html']\n",
      "773288.txt.aif.xml\n",
      "['773288.html']\n",
      "16.txt.aif.xml\n",
      "['16.html']\n",
      "5958.txt.aif.xml\n",
      "['5958.html']\n",
      "01_8-ok1.txt.aif.xml\n",
      "['01_8-ok1.html']\n",
      "بامرني14.txt.aif.xml\n",
      "['بامرني14.html']\n",
      "أمجد مدانات   مشترك    (2).txt.aif.xml\n",
      "['أمجد مدانات   مشترك    (2).html']\n",
      "ROA 15-3.txt.aif.xml\n",
      "['ROA 15-3.html']\n",
      "splitting emails\n",
      "8058-14632-1-PB.txt.aif.xml\n",
      "['8058-14632-1-PB.html']\n",
      "عامل و بسام.txt.aif.xml\n",
      "['عامل و بسام.html']\n",
      "9953685.txt.aif.xml\n",
      "['9953685.html']\n",
      "26.txt.aif.xml\n",
      "['26.html']\n",
      "splitting emails\n",
      "1555.txt.aif.xml\n",
      "['1555.html']\n",
      "أنس أبو عطا مشترك  ماجد العليمات    (10).txt.aif.xml\n",
      "['أنس أبو عطا مشترك  ماجد العليمات    (10).html']\n",
      "8.txt.aif.xml\n",
      "['8.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "string indices must be integers\n",
      "0 (9).txt.aif.xml\n",
      "['0 (9).html']\n",
      "splitting emails\n",
      "0 (8) a-eaa.txt.aif.xml\n",
      "['0 (8) a-eaa.html']\n",
      "ROA 15-9.txt.aif.xml\n",
      "['ROA 15-9.html']\n",
      "Roa Iktissadia17-6.txt.aif.xml\n",
      "['Roa Iktissadia17-6.html']\n",
      "2-a-e1.txt.aif.xml\n",
      "['2-a-e1.html']\n",
      "splitting emails\n",
      "0 (1)asd.txt.aif.xml\n",
      "['0 (1)asd.html']\n",
      "1 (1).txt.aif.xml\n",
      "['1 (1).html']\n",
      "ABPR_04_2_AR.txt.aif.xml\n",
      "['ABPR_04_2_AR.html']\n",
      "bab1.txt.aif.xml\n",
      "['bab1.html']\n",
      "4.txt.aif.xml\n",
      "['4.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "(5)اتجاهات طلبة المرحلة الأساسية إبراهيم الشرع.txt.aif.xml\n",
      "['(5)اتجاهات طلبة المرحلة الأساسية إبراهيم الشرع.html']\n",
      "R11-6.txt.aif.xml\n",
      "['R11-6.html']\n",
      "1 (2).txt.aif.xml\n",
      "['1 (2).html']\n",
      "01-15-ok04.txt.aif.xml\n",
      "['01-15-ok04.html']\n",
      "R11-11.txt.aif.xml\n",
      "['R11-11.html']\n",
      "خواص القرآن الكريم في الشفاء.txt.aif.xml\n",
      "['خواص القرآن الكريم في الشفاء.html']\n",
      "0 (6)abcd.txt.aif.xml\n",
      "['0 (6)abcd.html']\n",
      "د عائشة عهد.txt.aif.xml\n",
      "['د عائشة عهد.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "11-a-e1.txt.aif.xml\n",
      "['11-a-e1.html']\n",
      "splitting emails\n",
      "Roa Iktissadia17-12.txt.aif.xml\n",
      "['Roa Iktissadia17-12.html']\n",
      "splitting emails\n",
      "5-Extraction and Characterization of rapeseed.txt.aif.xml\n",
      "['5-Extraction and Characterization of rapeseed.html']\n",
      "10-a-e1.txt.aif.xml\n",
      "['10-a-e1.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "0 (9)aa.txt.aif.xml\n",
      "['0 (9)aa.html']\n",
      "20.txt.aif.xml\n",
      "['20.html']\n",
      "0 (5).txt.aif.xml\n",
      "['0 (5).html']\n",
      "splitting emails\n",
      "string indices must be integers\n",
      "4jgf.txt.aif.xml\n",
      "['4jgf.html']\n",
      "ROA 15-4.txt.aif.xml\n",
      "['ROA 15-4.html']\n",
      "0 (10)aa.txt.aif.xml\n",
      "['0 (10)aa.html']\n",
      "8044-14624-1-PB.txt.aif.xml\n",
      "['8044-14624-1-PB.html']\n",
      "773290.txt.aif.xml\n",
      "['773290.html']\n",
      "10.txt.aif.xml\n",
      "['10.html']\n",
      "8053-14629-1-PB.txt.aif.xml\n",
      "['8053-14629-1-PB.html']\n",
      "paper_ed7_34.txt.aif.xml\n",
      "['paper_ed7_34.html']\n",
      "19.txt.aif.xml\n",
      "['19.html']\n",
      "‫R11-5 ‫‬.txt.aif.xml\n",
      "['\\u202bR11-5 \\u202b\\u202c.html']\n",
      "wa2.txt.aif.xml\n",
      "['wa2.html']\n",
      "ROA 15-7.txt.aif.xml\n",
      "['ROA 15-7.html']\n",
      "splitting emails\n",
      "Effect of Hardening to drought  .txt.aif.xml\n",
      "['Effect of Hardening to drought  .html']\n",
      "5949.txt.aif.xml\n",
      "['5949.html']\n",
      "حسام 118-150.txt.aif.xml\n",
      "['حسام 118-150.html']\n",
      "(9)فاعلية التدريس ماهر الزيادات.txt.aif.xml\n",
      "['(9)فاعلية التدريس ماهر الزيادات.html']\n",
      "هارون الطورة  (9).txt.aif.xml\n",
      "['هارون الطورة  (9).html']\n",
      "8060-14635-1-PB.txt.aif.xml\n",
      "['8060-14635-1-PB.html']\n",
      "07.txt.aif.xml\n",
      "['07.html']\n",
      "13-a-e1.txt.aif.xml\n",
      "['13-a-e1.html']\n",
      "انتصار.txt.aif.xml\n",
      "['انتصار.html']\n",
      "wa7.txt.aif.xml\n",
      "['wa7.html']\n",
      "0 (13aa).txt.aif.xml\n",
      "['0 (13aa).html']\n",
      "‫R11-2 ‫‬.txt.aif.xml\n",
      "['\\u202bR11-2 \\u202b\\u202c.html']\n",
      "splitting emails\n",
      "4ff.txt.aif.xml\n",
      "['4ff.html']\n",
      "ROA 15-10.txt.aif.xml\n",
      "['ROA 15-10.html']\n",
      "0.txt.aif.xml\n",
      "['0.html']\n",
      "bab7.txt.aif.xml\n",
      "['bab7.html']\n",
      "splitting emails\n",
      "12-a-e1.txt.aif.xml\n",
      "['12-a-e1.html']\n",
      "splitting emails\n",
      "6 a-e.txt.aif.xml\n",
      "['6 a-e.html']\n",
      "Roa Iktissadia17-16.txt.aif.xml\n",
      "['Roa Iktissadia17-16.html']\n",
      "splitting emails\n",
      "12879-39192-1-SM.txt.aif.xml\n",
      "['12879-39192-1-SM.html']\n",
      "bab11.txt.aif.xml\n",
      "['bab11.html']\n",
      "ساجدة عزيز    عربي     (3.txt.aif.xml\n",
      "['ساجدة عزيز    عربي     (3.html']\n",
      "عبد الرحمن 88-117.txt.aif.xml\n",
      "['عبد الرحمن 88-117.html']\n",
      "بحث مستل مصطفى زيد عبد الله.txt.aif.xml\n",
      "['بحث مستل مصطفى زيد عبد الله.html']\n",
      "0 (4).txt.aif.xml\n",
      "['0 (4).html']\n",
      "5 a-e.txt.aif.xml\n",
      "['5 a-e.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "ROA 15-5.txt.aif.xml\n",
      "['ROA 15-5.html']\n",
      "'zone'\n",
      "Roa Iktissadia17-20.txt.aif.xml\n",
      "['Roa Iktissadia17-20.html']\n",
      "23.txt.aif.xml\n",
      "['23.html']\n",
      "splitting emails\n",
      "01-14-ok03.txt.aif.xml\n",
      "['01-14-ok03.html']\n",
      "4 a-ee.txt.aif.xml\n",
      "['4 a-ee.html']\n",
      "د احمد ادم احمد.txt.aif.xml\n",
      "['د احمد ادم احمد.html']\n",
      "four.txt.aif.xml\n",
      "['four.html']\n",
      "28.txt.aif.xml\n",
      "['28.html']\n",
      "33161.txt.aif.xml\n",
      "['33161.html']\n",
      "91237727.txt.aif.xml\n",
      "['91237727.html']\n",
      "ورقة رقم 5.txt.aif.xml\n",
      "['ورقة رقم 5.html']\n",
      "بورون - كفلر  الايبوكسي.txt.aif.xml\n",
      "['بورون - كفلر  الايبوكسي.html']\n",
      "ظاهرة الفقر والسوق غير الرسمي.txt.aif.xml\n",
      "['ظاهرة الفقر والسوق غير الرسمي.html']\n",
      "د محمدحسب الله ، احمد ادم.txt.aif.xml\n",
      "['د محمدحسب الله ، احمد ادم.html']\n",
      "splitting emails\n",
      "701688.txt.aif.xml\n",
      "['701688.html']\n",
      "قطر الندى3.txt.aif.xml\n",
      "['قطر الندى3.html']\n",
      "(6)  أحمد الطراونة.txt.aif.xml\n",
      "['(6)  أحمد الطراونة.html']\n",
      "nin.txt.aif.xml\n",
      "['nin.html']\n",
      "ROA 15-8.txt.aif.xml\n",
      "['ROA 15-8.html']\n",
      "Roa Iktissadia17-9.txt.aif.xml\n",
      "['Roa Iktissadia17-9.html']\n",
      "splitting emails\n",
      "0 (5)abcd.txt.aif.xml\n",
      "['0 (5)abcd.html']\n",
      "02.txt.aif.xml\n",
      "['02.html']\n",
      "9a color.txt.aif.xml\n",
      "['9a color.html']\n",
      "3hjgf.txt.aif.xml\n",
      "['3hjgf.html']\n",
      "1 (22).txt.aif.xml\n",
      "['1 (22).html']\n",
      "1 (11).txt.aif.xml\n",
      "['1 (11).html']\n",
      "صادق الحايك   (9).txt.aif.xml\n",
      "['صادق الحايك   (9).html']\n",
      "0 (14)aa.txt.aif.xml\n",
      "['0 (14)aa.html']\n",
      "لطفي الخطيب    8.txt.aif.xml\n",
      "['لطفي الخطيب    8.html']\n",
      "0 (5)aa.txt.aif.xml\n",
      "['0 (5)aa.html']\n",
      "2file.txt.aif.xml\n",
      "['2file.html']\n",
      "16 فخرية.txt.aif.xml\n",
      "['16 فخرية.html']\n",
      "(5) محمد العزام.txt.aif.xml\n",
      "['(5) محمد العزام.html']\n",
      "3 a-e.txt.aif.xml\n",
      "['3 a-e.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "0 (9)abcd.txt.aif.xml\n",
      "['0 (9)abcd.html']\n",
      "8061-14636-1-PB.txt.aif.xml\n",
      "['8061-14636-1-PB.html']\n",
      "Roa Iktissadia17-10.txt.aif.xml\n",
      "['Roa Iktissadia17-10.html']\n",
      "splitting emails\n",
      "6.txt.aif.xml\n",
      "['6.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "7ff.txt.aif.xml\n",
      "['7ff.html']\n",
      "splitting emails\n",
      "91259885.txt.aif.xml\n",
      "['91259885.html']\n",
      "splitting emails\n",
      "bab3.txt.aif.xml\n",
      "['bab3.html']\n",
      "1 (18).txt.aif.xml\n",
      "['1 (18).html']\n",
      "أحمد صومان   (3)  مس.txt.aif.xml\n",
      "['أحمد صومان   (3)  مس.html']\n",
      "ابراهيم الطراونه  (9).txt.aif.xml\n",
      "['ابراهيم الطراونه  (9).html']\n",
      "7.txt.aif.xml\n",
      "['7.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "محمد غزيوات   (1).txt.aif.xml\n",
      "['محمد غزيوات   (1).html']\n",
      "عزة خير الدين.txt.aif.xml\n",
      "['عزة خير الدين.html']\n",
      "اعلم.txt.aif.xml\n",
      "['اعلم.html']\n",
      "wa4.txt.aif.xml\n",
      "['wa4.html']\n",
      "Roa Iktissadia17-23.txt.aif.xml\n",
      "['Roa Iktissadia17-23.html']\n",
      "paper_ed2_33.txt.aif.xml\n",
      "['paper_ed2_33.html']\n",
      "3.txt.aif.xml\n",
      "['3.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "0 (2).txt.aif.xml\n",
      "['0 (2).html']\n",
      "splitting emails\n",
      "4 a-e.txt.aif.xml\n",
      "['4 a-e.html']\n",
      "splitting emails\n",
      "'.T43'\n",
      "ناظم 151-179.txt.aif.xml\n",
      "['ناظم 151-179.html']\n",
      "17.txt.aif.xml\n",
      "['17.html']\n",
      "splitting emails\n",
      "0 (17)aa.txt.aif.xml\n",
      "['0 (17)aa.html']\n",
      "Roa Iktissadia17-8.txt.aif.xml\n",
      "['Roa Iktissadia17-8.html']\n",
      "splitting emails\n",
      "27.txt.aif.xml\n",
      "['27.html']\n",
      "10-رغيد.txt.aif.xml\n",
      "['10-رغيد.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "محمد بديوي بني ملحم   (5).txt.aif.xml\n",
      "['محمد بديوي بني ملحم   (5).html']\n",
      "index 0 is out of bounds for axis 0 with size 0\n",
      "9.txt.aif.xml\n",
      "['9.html']\n",
      "5950.txt.aif.xml\n",
      "['5950.html']\n",
      "5944.txt.aif.xml\n",
      "['5944.html']\n",
      "Roa Iktissadia17-1.txt.aif.xml\n",
      "['Roa Iktissadia17-1.html']\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "splitting emails\n",
      "14.txt.aif.xml\n",
      "['14.html']\n",
      "الثقة والاتجاه.txt.aif.xml\n",
      "['الثقة والاتجاه.html']\n",
      "Roa Iktissadia17-22.txt.aif.xml\n",
      "['Roa Iktissadia17-22.html']\n",
      "‫R11-4.txt.aif.xml\n",
      "['\\u202bR11-4.html']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "د طارق الدليمي.txt.aif.xml\n",
      "['د طارق الدليمي.html']\n",
      "5945.txt.aif.xml\n",
      "['5945.html']\n",
      "3ffff.txt.aif.xml\n",
      "['3ffff.html']\n",
      "منى محمود خضير 1.txt.aif.xml\n",
      "['منى محمود خضير 1.html']\n",
      "8056-14630-1-PB.txt.aif.xml\n",
      "['8056-14630-1-PB.html']\n"
     ]
    }
   ],
   "source": [
    "## Join features from html to labels using scripts below\n",
    "\n",
    "all_training_data_dfs = []\n",
    "files_not_loaded = []\n",
    "for l,f in list(dict_labels_to_html.items()):\n",
    "    print(l)\n",
    "    print(f)\n",
    "    path_labels_file = 'labels/'+l\n",
    "    path_html_file = 'new_html_files/'+f[0]\n",
    "    \n",
    "    EL = E_L.ExtractLabels(path_labels_file)\n",
    "    EF = E_F.ExtractFeatures(path_html_file,'data/all_ara_names.txt','data/all_eng_names.txt',\n",
    "                         'data/ara_aff.txt','data/eng_aff.txt')\n",
    "    try:\n",
    "        labels_df = EL.get_labels()\n",
    "        features_df = EF.get_all_features()\n",
    "        JFL = J_F_L.JoinFeaturesLabels(features_df,labels_df)\n",
    "        df = JFL.join()\n",
    "        all_training_data_dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        files_not_loaded.append(l)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_training_data_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## remove empty dataframes\n",
    "all_training_data_dfs = [x for x in all_training_data_dfs if len(x) >0]\n",
    "len(all_training_data_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## remove dataframes with only two texts\n",
    "all_training_data_dfs = [x for x in all_training_data_dfs if len(x) >3]\n",
    "len(all_training_data_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save dfs into pickle file\n",
    "import pickle\n",
    "with open('all_training_data_dfs_updated.txt', 'wb') as fp:\n",
    "    pickle.dump(all_training_data_dfs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load dfs into pickle file\n",
    "import pickle\n",
    "with open('all_training_data_dfs_updated.txt', \"rb\") as f:\n",
    "    all_training_data_dfs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if there are other_titles with ara text\n",
    "i=0\n",
    "dfs_to_del = []\n",
    "for df in all_training_data_dfs:\n",
    "    if len(df[df['sub_label'] == 'other_title']) > 0:\n",
    "        try:\n",
    "            for ff in df[df['sub_label'] == 'other_title']['is_ara']:\n",
    "                if ff == '1:\n",
    "                    dfs_to_del.append(i)\n",
    "#             if df[df['sub_label'] == 'main_authors']['is_ara'].values[0] == 0:\n",
    "#                 dfs_to_del.append(i)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[178, 269]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dfs_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = 269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_training_data_dfs[dd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>labels</th>\n",
       "      <th>sub_label</th>\n",
       "      <th>is_ara</th>\n",
       "      <th>direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>تأثير النحاس على البروتينات والأنزيمات في الحي...</td>\n",
       "      <td>metadata</td>\n",
       "      <td>main_title</td>\n",
       "      <td>1</td>\n",
       "      <td>rtl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dreissena polymorpha</td>\n",
       "      <td>metadata</td>\n",
       "      <td>main_title</td>\n",
       "      <td>0</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>عبد علي ذاكر</td>\n",
       "      <td>metadata</td>\n",
       "      <td>main_authors</td>\n",
       "      <td>1</td>\n",
       "      <td>rtl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>و</td>\n",
       "      <td>other</td>\n",
       "      <td>unkown</td>\n",
       "      <td>0</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>احمد سامي فرحان</td>\n",
       "      <td>metadata</td>\n",
       "      <td>main_authors</td>\n",
       "      <td>1</td>\n",
       "      <td>rtl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قسم علوم الحياة- كلية العلوم- جامعة الانبار</td>\n",
       "      <td>metadata</td>\n",
       "      <td>main_affiliation</td>\n",
       "      <td>1</td>\n",
       "      <td>rtl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>الخلاصة</td>\n",
       "      <td>metadata</td>\n",
       "      <td>main_abstract</td>\n",
       "      <td>1</td>\n",
       "      <td>rtl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Effect of copper on soluble proteins and enzym...</td>\n",
       "      <td>metadata</td>\n",
       "      <td>other_title</td>\n",
       "      <td>0</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Abid A. Thaker</td>\n",
       "      <td>metadata</td>\n",
       "      <td>other_authors</td>\n",
       "      <td>0</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ahmad S. Farhan</td>\n",
       "      <td>metadata</td>\n",
       "      <td>other_authors</td>\n",
       "      <td>0</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Department of Biology، College of Science، Uni...</td>\n",
       "      <td>metadata</td>\n",
       "      <td>other_affiliation</td>\n",
       "      <td>0</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Summary</td>\n",
       "      <td>metadata</td>\n",
       "      <td>other_abstract</td>\n",
       "      <td>0</td>\n",
       "      <td>ltr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              level_0    labels  \\\n",
       "0   تأثير النحاس على البروتينات والأنزيمات في الحي...  metadata   \n",
       "1                                Dreissena polymorpha  metadata   \n",
       "2                                        عبد علي ذاكر  metadata   \n",
       "3                                                   و     other   \n",
       "4                                     احمد سامي فرحان  metadata   \n",
       "5        قسم علوم الحياة- كلية العلوم- جامعة الانبار   metadata   \n",
       "7                                             الخلاصة  metadata   \n",
       "9   Effect of copper on soluble proteins and enzym...  metadata   \n",
       "10                                     Abid A. Thaker  metadata   \n",
       "11                                    Ahmad S. Farhan  metadata   \n",
       "20  Department of Biology، College of Science، Uni...  metadata   \n",
       "21                                            Summary  metadata   \n",
       "\n",
       "            sub_label  is_ara direction  \n",
       "0          main_title       1       rtl  \n",
       "1          main_title       0       ltr  \n",
       "2        main_authors       1       rtl  \n",
       "3              unkown       0       ltr  \n",
       "4        main_authors       1       rtl  \n",
       "5    main_affiliation       1       rtl  \n",
       "7       main_abstract       1       rtl  \n",
       "9         other_title       0       ltr  \n",
       "10      other_authors       0       ltr  \n",
       "11      other_authors       0       ltr  \n",
       "20  other_affiliation       0       ltr  \n",
       "21     other_abstract       0       ltr  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_training_data_dfs[dd][['level_0','labels','sub_label','is_ara']].head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_data_dfs[dd].iloc[1,all_training_data_dfs[dd].columns.get_loc('is_ara')] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_training_data_dfs_updated.txt', 'wb') as fp:\n",
    "    pickle.dump(all_training_data_dfs, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if certain aff not in aff list and add it \n",
    "#aff?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/ara_aff.txt', \"rb\") as f:\n",
    "    ara_aff = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'جامعة يحي فارس المدية' in ara_aff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ara_aff = ara_aff + ['جامعة يحي فارس المدية']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2720"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ara_aff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2720"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ara_aff= list(set(ara_aff))\n",
    "len(ara_aff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/ara_aff.txt', 'wb') as fp:\n",
    "    pickle.dump(ara_aff, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if certain name not in names list and add it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/all_ara_names.txt', \"rb\") as f:\n",
    "    ara_names = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'جعفر' in ara_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "ara_names = ara_names +[\"أياد\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16208"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ara_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16208"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ara_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/all_ara_names.txt', 'wb') as fp:\n",
    "    pickle.dump(list(set(ara_names)), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/all_eng_names.txt', \"rb\") as f:\n",
    "    eng_names = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Naouel\".title() in eng_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18797"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_names = eng_names +['MELLAHI'.title(),'ZOUGGAR'.title()]\n",
    "len(eng_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18796"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(eng_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/all_eng_names.txt', 'wb') as fp:\n",
    "    pickle.dump(list(set(eng_names)), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature of row before in each doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for features_df in all_training_data_dfs:\n",
    "    \n",
    "    if 'is_footnote' in features_df.columns:\n",
    "        values_to_add = features_df.iloc[len(features_df)-1][['text-align','font-size','font-family','direction',\n",
    "                               'zone_word_count','text-indent',\n",
    "                               'contains_ara_authors','contains_eng_authors',\n",
    "                               'contains_email','contains_ara_aff',\n",
    "                               'contains_eng_aff','is_single_word',\n",
    "                               'is_footnote','labels','sub_label','is_max_font',\n",
    "                               'contains_other_keywords','contains_main_keywords',\n",
    "                               'contains_date_format','contains_main_abstract',\n",
    "                               'contains_other_abstract','contains_date_keywords',\n",
    "                               'starts_with_nb',\n",
    "                               'contains_url','contains_ara_aff_keywords',\n",
    "                               'contains_eng_aff_keywords','count_aff_keywords',\n",
    "                               'contains_eng_aff_tags','contains_ara_aff_tags',\n",
    "                               'count_aff_tags','tables_figures_keywords','relative_count_punctuation',\n",
    "                               'contains_abs_is_single','reference_keyword','contains_mag_keywords',\n",
    "                               'contains_headings','contains_reference_heading',\n",
    "                               'perc_eng_aff','perc_ara_aff']].copy(deep=True).values\n",
    "\n",
    "        features_df.loc[features_df.index[0],['text-align_b','font-size_b','font-family_b',\"direction_b\",\n",
    "                               'zone_word_count_b','text-indent_b',\n",
    "                               'contains_ara_authors_b','contains_eng_authors_b',\n",
    "                               'contains_email_b','contains_ara_aff_b',\n",
    "                               'contains_eng_aff_b','is_single_word_b',\n",
    "                               'is_footnote_b','labels_b','sub_label_b','is_max_font_b',\n",
    "                               'contains_other_keywords_b','contains_main_keywords_b',\n",
    "                               'contains_date_format_b','contains_main_abstract_b',\n",
    "                               'contains_other_abstract_b','contains_date_keywords_b',\n",
    "                               'starts_with_nb_b',\n",
    "                               'contains_url_b','contains_ara_aff_keywords_b',\n",
    "                               'contains_eng_aff_keywords_b','count_aff_keywords_b',\n",
    "                               'contains_eng_aff_tags_b','contains_ara_aff_tags_b',\n",
    "                               'count_aff_tags_b','tables_figures_keywords_b','relative_count_punctuation_b',\n",
    "                               'contains_abs_is_single_b','reference_keyword_b','contains_mag_keywords_b',\n",
    "                               'contains_headings_b','contains_reference_heading_b',\n",
    "                               'perc_eng_aff_b','perc_ara_aff_b']] = values_to_add\n",
    "    else:\n",
    "        values_to_add = features_df.iloc[len(features_df)-1][['text-align','font-size','font-family','direction',\n",
    "                               'zone_word_count','text-indent',\n",
    "                               'contains_ara_authors','contains_eng_authors',\n",
    "                               'contains_email','contains_ara_aff',\n",
    "                               'contains_eng_aff','is_single_word',\n",
    "                               'labels','sub_label','is_max_font','contains_other_keywords',\n",
    "                               'contains_main_keywords','contains_date_format',\n",
    "                               'contains_main_abstract','contains_other_abstract',\n",
    "                               'contains_date_keywords',\n",
    "                               'starts_with_nb','contains_url','contains_ara_aff_keywords',\n",
    "                               'contains_eng_aff_keywords','count_aff_keywords',\n",
    "                               'contains_eng_aff_tags','contains_ara_aff_tags',\n",
    "                               'count_aff_tags','tables_figures_keywords','relative_count_punctuation',\n",
    "                               'contains_abs_is_single','reference_keyword','contains_mag_keywords',\n",
    "                               'contains_headings','contains_reference_heading',\n",
    "                               'perc_eng_aff','perc_ara_aff']].copy(deep=True).values\n",
    "\n",
    "        features_df.loc[features_df.index[0],['text-align_b','font-size_b','font-family_b',\"direction_b\",\n",
    "                               'zone_word_count_b','text-indent_b',\n",
    "                               'contains_ara_authors_b','contains_eng_authors_b',\n",
    "                               'contains_email_b','contains_ara_aff_b',\n",
    "                               'contains_eng_aff_b','is_single_word_b',\n",
    "                               'labels_b','sub_label_b','is_max_font_b','contains_other_keywords_b',\n",
    "                               'contains_main_keywords_b','contains_date_format_b',\n",
    "                               'contains_main_abstract_b','contains_other_abstract_b',\n",
    "                               'contains_date_keywords_b',\n",
    "                               'starts_with_nb_b','contains_url_b','contains_ara_aff_keywords_b',\n",
    "                               'contains_eng_aff_keywords_b','count_aff_keywords_b',\n",
    "                               'contains_eng_aff_tags_b','contains_ara_aff_tags_b',\n",
    "                               'count_aff_tags_b','tables_figures_keywords_b','relative_count_punctuation_b',\n",
    "                               'contains_abs_is_single_b','reference_keyword_b','contains_mag_keywords_b',\n",
    "                               'contains_headings_b','contains_reference_heading_b',\n",
    "                               'perc_eng_aff_b','perc_ara_aff_b']] = values_to_add\n",
    "    for i in range(1,len(features_df)):\n",
    "\n",
    "        if 'is_footnote' in features_df.columns:\n",
    "            values_to_add = features_df.iloc[i-1][['text-align','font-size','font-family','direction',\n",
    "                               'zone_word_count','text-indent',\n",
    "                               'contains_ara_authors','contains_eng_authors',\n",
    "                               'contains_email','contains_ara_aff',\n",
    "                               'contains_eng_aff','is_single_word',\n",
    "                               'is_footnote','labels','sub_label','is_max_font',\n",
    "                               'contains_other_keywords','contains_main_keywords',\n",
    "                               'contains_date_format','contains_main_abstract',\n",
    "                               'contains_other_abstract','contains_date_keywords',\n",
    "                               'starts_with_nb',\n",
    "                               'contains_url','contains_ara_aff_keywords',\n",
    "                               'contains_eng_aff_keywords','count_aff_keywords',\n",
    "                               'contains_eng_aff_tags','contains_ara_aff_tags',\n",
    "                               'count_aff_tags','tables_figures_keywords','relative_count_punctuation',\n",
    "                               'contains_abs_is_single','reference_keyword','contains_mag_keywords',\n",
    "                               'contains_headings','contains_reference_heading',\n",
    "                               'perc_eng_aff','perc_ara_aff']].copy(deep=True).values\n",
    "\n",
    "            features_df.loc[features_df.index[i],['text-align_b','font-size_b','font-family_b',\"direction_b\",\n",
    "                               'zone_word_count_b','text-indent_b',\n",
    "                               'contains_ara_authors_b','contains_eng_authors_b',\n",
    "                               'contains_email_b','contains_ara_aff_b',\n",
    "                               'contains_eng_aff_b','is_single_word_b',\n",
    "                               'is_footnote_b','labels_b','sub_label_b','is_max_font_b',\n",
    "                               'contains_other_keywords_b','contains_main_keywords_b',\n",
    "                               'contains_date_format_b','contains_main_abstract_b',\n",
    "                               'contains_other_abstract_b','contains_date_keywords_b',\n",
    "                               'starts_with_nb_b',\n",
    "                               'contains_url_b','contains_ara_aff_keywords_b',\n",
    "                               'contains_eng_aff_keywords_b','count_aff_keywords_b',\n",
    "                               'contains_eng_aff_tags_b','contains_ara_aff_tags_b',\n",
    "                               'count_aff_tags_b','tables_figures_keywords_b','relative_count_punctuation_b',\n",
    "                               'contains_abs_is_single_b','reference_keyword_b','contains_mag_keywords_b',\n",
    "                               'contains_headings_b','contains_reference_heading_b',\n",
    "                               'perc_eng_aff_b','perc_ara_aff_b']] = values_to_add\n",
    "        else:\n",
    "            values_to_add = features_df.iloc[i-1][['text-align','font-size','font-family','direction',\n",
    "                               'zone_word_count','text-indent',\n",
    "                               'contains_ara_authors','contains_eng_authors',\n",
    "                               'contains_email','contains_ara_aff',\n",
    "                               'contains_eng_aff','is_single_word',\n",
    "                               'labels','sub_label','is_max_font','contains_other_keywords',\n",
    "                               'contains_main_keywords','contains_date_format',\n",
    "                               'contains_main_abstract','contains_other_abstract',\n",
    "                               'contains_date_keywords',\n",
    "                               'starts_with_nb','contains_url','contains_ara_aff_keywords',\n",
    "                               'contains_eng_aff_keywords','count_aff_keywords',\n",
    "                               'contains_eng_aff_tags','contains_ara_aff_tags',\n",
    "                               'count_aff_tags','tables_figures_keywords','relative_count_punctuation',\n",
    "                               'contains_abs_is_single','reference_keyword','contains_mag_keywords',\n",
    "                               'contains_headings','contains_reference_heading',\n",
    "                               'perc_eng_aff','perc_ara_aff']].copy(deep=True).values\n",
    "\n",
    "            features_df.loc[features_df.index[i],['text-align_b','font-size_b','font-family_b',\"direction_b\",\n",
    "                               'zone_word_count_b','text-indent_b',\n",
    "                               'contains_ara_authors_b','contains_eng_authors_b',\n",
    "                               'contains_email_b','contains_ara_aff_b',\n",
    "                               'contains_eng_aff_b','is_single_word_b',\n",
    "                               'labels_b','sub_label_b','is_max_font_b','contains_other_keywords_b',\n",
    "                               'contains_main_keywords_b','contains_date_format_b',\n",
    "                               'contains_main_abstract_b','contains_other_abstract_b',\n",
    "                               'contains_date_keywords_b',\n",
    "                               'starts_with_nb_b','contains_url_b','contains_ara_aff_keywords_b',\n",
    "                               'contains_eng_aff_keywords_b','count_aff_keywords_b',\n",
    "                               'contains_eng_aff_tags_b','contains_ara_aff_tags_b',\n",
    "                               'count_aff_tags_b','tables_figures_keywords_b','relative_count_punctuation_b',\n",
    "                               'contains_abs_is_single_b','reference_keyword_b','contains_mag_keywords_b',\n",
    "                               'contains_headings_b','contains_reference_heading_b',\n",
    "                               'perc_eng_aff_b','perc_ara_aff_b']] = values_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save dfs after update\n",
    "with open('all_training_data_dfs_updated.txt', 'wb') as fp:\n",
    "    pickle.dump(all_training_data_dfs, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_training_data_dfs_updated.txt', \"rb\") as f:\n",
    "    all_training_data_dfs = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV,train_test_split,KFold,StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs_concat = pd.concat(all_training_data_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fill empty cells\n",
    "all_dfs_concat['is_footnote'].fillna(0,inplace = True)\n",
    "all_dfs_concat['is_footnote_b'].fillna(0,inplace = True)\n",
    "\n",
    "## remove inches \"in\" from indentation\n",
    "all_dfs_concat['text-indent'] = all_dfs_concat['text-indent'].astype(str).apply(lambda x: x.split('in')[0].strip())\n",
    "all_dfs_concat['text-indent_b'] = all_dfs_concat['text-indent'].astype(str).apply(lambda x: x.split('in')[0].strip())\n",
    "\n",
    "## change fon to int from str\n",
    "all_dfs_concat['font-size'] = all_dfs_concat['font-size'].astype(int)\n",
    "\n",
    "## hot-encode certain features\n",
    "all_dfs_concat = pd.get_dummies(all_dfs_concat, columns = ['text-align','font-family','direction','text-align_b','font-family_b','direction_b',\n",
    "                                                           'labels_b','sub_label_b'])\n",
    "# fill empty cells\n",
    "all_dfs_concat.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "all_dfs_concat = all_dfs_concat.fillna(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove empty strings\n",
    "all_dfs_concat = all_dfs_concat[all_dfs_concat['level_0'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check if there are null values\n",
    "all_dfs_concat.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_dfs_concat = all_dfs_concat.drop(['contains_ara_aff_b','contains_eng_aff_b',\n",
    "                                   'in_body'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save dataframe that conatins all training data and there features \n",
    "all_dfs_concat.to_csv('full_dataset7.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs_concat = pd.read_csv('full_dataset7.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seperate into labels and features\n",
    "\n",
    "all_labels = all_dfs_concat['labels']\n",
    "all_features = all_dfs_concat.drop(['level_0','sub_label','labels','sub_label_b_acknowledgment',\n",
    "                                   'sub_label_b_bib_info',\n",
    "                                   'sub_label_b_copyright','sub_label_b_correspondence',\n",
    "                                   'sub_label_b_dates', 'sub_label_b_equation', 'sub_label_b_figure',\n",
    "                                   'sub_label_b_figure_heading','sub_label_b_main_abstract',\n",
    "                                   'sub_label_b_main_affiliation','sub_label_b_main_authors',\n",
    "                                   'sub_label_b_main_keywords','sub_label_b_main_title',\n",
    "                                   'sub_label_b_other_abstract','sub_label_b_other_affiliation',\n",
    "                                   'sub_label_b_other_authors','sub_label_b_other_keywords',\n",
    "                                   'sub_label_b_other_title','sub_label_b_page_number',\n",
    "                                   'sub_label_b_references','sub_label_b_table',\n",
    "                                   'sub_label_b_table_heading','sub_label_b_text','sub_label_b_unknown',\n",
    "                                    'starts_with_nb','starts_with_nb_b'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "body          12452\n",
       "metadata       4268\n",
       "references     1985\n",
       "other           440\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.05,random_state = 114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Standardize features\n",
    "scaler = StandardScaler().fit(all_features)\n",
    "X_train_std = scaler.transform(all_features)\n",
    "# X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save features\n",
    "filename = 'first_model_scaler.sav'\n",
    "pickle.dump(scaler, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt Several models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9665513264129181\n",
      "F-1 Score: [0.97826087 0.97387173 0.31578947 0.94736842]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F-1 Score:\", metrics.f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs',penalty='l2', C=0.1)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "n_scores = cross_val_score(model, X_train_std, all_labels, scoring='accuracy', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.963125  , 0.95      , 0.9625    , 0.959375  , 0.955     ,\n",
       "       0.95625   , 0.9625    , 0.96125   , 0.9625    , 0.9518449 ,\n",
       "       0.95375   , 0.95375   , 0.955625  , 0.950625  , 0.959375  ,\n",
       "       0.96375   , 0.965625  , 0.960625  , 0.96375   , 0.9587242 ,\n",
       "       0.959375  , 0.9625    , 0.95375   , 0.96375   , 0.968125  ,\n",
       "       0.9525    , 0.960625  , 0.954375  , 0.9575    , 0.95809881])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(l1_ratio=0.9, multi_class='multinomial',\n",
       "                   penalty='elasticnet', solver='saga')"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(multi_class='multinomial', solver='saga',penalty='elasticnet',l1_ratio=0.9)\n",
    "model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9642445213379469\n",
      "F-1 Score: [0.97822142 0.97841727 0.34782609 0.92708333]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F-1 Score:\", metrics.f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=300, random_state=1)"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(solver='adam',random_state=1, max_iter=300)\n",
    "mlp.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9677047289504037\n",
      "F-1 Score: [0.97996357 0.98086124 0.43478261 0.93333333]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F-1 Score:\", metrics.f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        body       0.97      0.99      0.98       546\n",
      "    metadata       0.98      0.98      0.98       209\n",
      "       other       0.71      0.31      0.43        16\n",
      "  references       0.92      0.95      0.93        96\n",
      "\n",
      "    accuracy                           0.97       867\n",
      "   macro avg       0.90      0.81      0.83       867\n",
      "weighted avg       0.97      0.97      0.96       867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.1, max_iter=300, random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(solver='lbfgs',alpha = 0.1,random_state=1, max_iter=300)\n",
    "mlp.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9642445213379469\n",
      "F-1 Score: [0.97996357 0.97831325 0.30769231 0.93333333]\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp.predict(X_test_std)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F-1 Score:\", metrics.f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.31622777, max_iter=300, random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(solver='lbfgs',alpha = 0.31622777,random_state=1, max_iter=300)\n",
    "mlp.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9642445213379469\n",
      "F-1 Score: [0.97716895 0.98557692 0.33333333 0.92462312]\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp.predict(X_test_std)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F-1 Score:\", metrics.f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CURRENT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=10, max_iter=300, random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CURRENT MODEL\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(solver='lbfgs',alpha = 10,random_state=1, max_iter=300)\n",
    "mlp.fit(X_train_std, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9626865671641791\n",
      "F-1 Score: [0.98029557 0.97321429 0.59459459 0.89017341]\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp.predict(X_test_std)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F-1 Score:\", metrics.f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'first_model.sav'\n",
    "pickle.dump(mlp, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "strtfdKFold = StratifiedKFold(n_splits=10)\n",
    "kfold = strtfdKFold.split(all_features, all_labels)\n",
    "scores = []\n",
    "clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        body       0.97      0.99      0.98      1084\n",
      "    metadata       0.97      0.97      0.97       429\n",
      "       other       0.81      0.41      0.54        32\n",
      "  references       0.93      0.91      0.92       188\n",
      "\n",
      "    accuracy                           0.96      1733\n",
      "   macro avg       0.92      0.82      0.85      1733\n",
      "weighted avg       0.96      0.96      0.96      1733\n",
      "\n",
      "1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        body       0.98      0.98      0.98      1084\n",
      "    metadata       0.96      0.98      0.97       429\n",
      "       other       0.52      0.44      0.47        32\n",
      "  references       0.93      0.91      0.92       188\n",
      "\n",
      "    accuracy                           0.96      1733\n",
      "   macro avg       0.85      0.83      0.84      1733\n",
      "weighted avg       0.96      0.96      0.96      1733\n",
      "\n",
      "2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        body       0.99      0.98      0.98      1084\n",
      "    metadata       0.96      0.99      0.97       429\n",
      "       other       0.91      0.62      0.74        32\n",
      "  references       0.92      0.96      0.94       188\n",
      "\n",
      "    accuracy                           0.97      1733\n",
      "   macro avg       0.94      0.89      0.91      1733\n",
      "weighted avg       0.97      0.97      0.97      1733\n",
      "\n",
      "3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        body       0.98      0.92      0.95      1083\n",
      "    metadata       0.81      0.98      0.89       429\n",
      "       other       0.83      0.31      0.45        32\n",
      "  references       0.97      0.93      0.95       189\n",
      "\n",
      "    accuracy                           0.93      1733\n",
      "   macro avg       0.90      0.79      0.81      1733\n",
      "weighted avg       0.93      0.93      0.92      1733\n",
      "\n",
      "4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        body       0.95      0.98      0.97      1083\n",
      "    metadata       0.94      0.98      0.96       429\n",
      "       other       0.94      0.47      0.62        32\n",
      "  references       0.90      0.72      0.80       189\n",
      "\n",
      "    accuracy                           0.94      1733\n",
      "   macro avg       0.93      0.79      0.84      1733\n",
      "weighted avg       0.94      0.94      0.94      1733\n",
      "\n",
      "5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        body       0.96      0.98      0.97      1084\n",
      "    metadata       0.97      0.98      0.97       428\n",
      "       other       0.58      0.22      0.32        32\n",
      "  references       0.93      0.88      0.90       189\n",
      "\n",
      "    accuracy                           0.95      1733\n",
      "   macro avg       0.86      0.76      0.79      1733\n",
      "weighted avg       0.95      0.95      0.95      1733\n",
      "\n",
      "6\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        body       0.98      0.99      0.98      1084\n",
      "    metadata       0.98      0.99      0.98       428\n",
      "       other       0.94      0.53      0.68        32\n",
      "  references       0.94      0.94      0.94       189\n",
      "\n",
      "    accuracy                           0.97      1733\n",
      "   macro avg       0.96      0.86      0.90      1733\n",
      "weighted avg       0.97      0.97      0.97      1733\n",
      "\n",
      "7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        body       0.89      0.99      0.94      1084\n",
      "    metadata       0.97      0.99      0.98       428\n",
      "       other       0.90      0.29      0.44        31\n",
      "  references       0.91      0.37      0.53       189\n",
      "\n",
      "    accuracy                           0.91      1732\n",
      "   macro avg       0.92      0.66      0.72      1732\n",
      "weighted avg       0.91      0.91      0.89      1732\n",
      "\n",
      "8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        body       0.97      0.98      0.98      1084\n",
      "    metadata       0.94      0.97      0.96       428\n",
      "       other       0.92      0.39      0.55        31\n",
      "  references       0.95      0.90      0.93       189\n",
      "\n",
      "    accuracy                           0.96      1732\n",
      "   macro avg       0.95      0.81      0.85      1732\n",
      "weighted avg       0.96      0.96      0.96      1732\n",
      "\n",
      "9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        body       0.98      0.95      0.97      1084\n",
      "    metadata       0.87      0.98      0.92       428\n",
      "       other       0.80      0.26      0.39        31\n",
      "  references       0.91      0.92      0.91       189\n",
      "\n",
      "    accuracy                           0.94      1732\n",
      "   macro avg       0.89      0.78      0.80      1732\n",
      "weighted avg       0.94      0.94      0.94      1732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, (train, test) in enumerate(kfold):\n",
    "    X_k_train = all_features.iloc[train, :]\n",
    "    y_k_train = all_labels.iloc[train]\n",
    "    X_K_test = all_features.iloc[test,:]\n",
    "    y_K_test = all_labels.iloc[test]\n",
    "    \n",
    "#     X_k_train, y_k_train = pipeline.fit_resample(X_k_train, y_k_train)\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_k_train)\n",
    "    X_train_std = scaler.transform(X_k_train)\n",
    "    X_test_std = scaler.transform(X_K_test)\n",
    "    \n",
    "    clf.fit(X_train_std, y_k_train)\n",
    "    y_pred = clf.predict(X_test_std)\n",
    "    scores.append(metrics.accuracy_score(y_K_test, y_pred))\n",
    "    \n",
    "    scores.append(metrics.f1_score(y_K_test, y_pred, average=None))\n",
    "    \n",
    "    print(k)\n",
    "    print(metrics.classification_report(y_K_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507681682434044"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(scores[0::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8303030979369554"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores[1::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = all_dfs_concat[all_dfs_concat['labels'] == 'metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_abstract        771\n",
       "other_abstract       741\n",
       "main_authors         458\n",
       "main_affiliation     433\n",
       "main_title           409\n",
       "dates                272\n",
       "other_authors        258\n",
       "other_title          228\n",
       "main_keywords        163\n",
       "bib_info             151\n",
       "other_keywords       149\n",
       "other_affiliation    124\n",
       "correspondence       111\n",
       "Name: sub_label, dtype: int64"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['sub_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Drop Values where bib_info is given to title\n",
    "metadata_df = metadata_df.drop([597,722,2917,2921,3070,3071,3308,3327,3400,3401,3437,3835,4474,5177,8135,8461,9656,10380,10389,11527,12735,14662,16346,17177,17178,18361,18470,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_abstract        771\n",
       "other_abstract       741\n",
       "main_authors         458\n",
       "main_affiliation     433\n",
       "main_title           409\n",
       "dates                272\n",
       "other_authors        258\n",
       "other_title          228\n",
       "main_keywords        163\n",
       "other_keywords       149\n",
       "bib_info             124\n",
       "other_affiliation    124\n",
       "correspondence       111\n",
       "Name: sub_label, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['sub_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = metadata_df['sub_label']\n",
    "all_features = metadata_df.drop(['sub_label','labels','level_0',\n",
    "                                 'labels_b_body','labels_b_metadata','labels_b_other',\n",
    "                                'labels_b_references',\n",
    "                               'sub_label_b_acknowledgment','sub_label_b_copyright',\n",
    "                               'sub_label_b_equation', 'sub_label_b_figure',\n",
    "                               'sub_label_b_figure_heading','sub_label_b_page_number',\n",
    "                               'sub_label_b_references','sub_label_b_table',\n",
    "                               'sub_label_b_table_heading','sub_label_b_text',\n",
    "                               'sub_label_b_unknown','starts_with_nb','starts_with_nb_b',],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_features, all_labels, test_size=0.25,random_state = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/imblearn/utils/_validation.py:587: FutureWarning: Pass sampling_strategy=not majority as keyword args. From version 0.9 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Over Sample data to remoive imbalance\n",
    "over = SMOTE('not majority')\n",
    "X, y = over.fit_resample(all_features, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10023, 184)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X_train_std = scaler.transform(X)\n",
    "# X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'metadata_model_scaler.sav'\n",
    "pickle.dump(scaler, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(X_train_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.914179104477612\n",
      "F-1 Score: [0.71641791 0.93939394 0.98591549 0.91076923 0.90666667 0.88333333\n",
      " 0.95081967 0.85308057 0.94906166 0.92307692 0.94193548 0.94736842\n",
      " 0.92753623]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F-1 Score:\", metrics.f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, multi_class='multinomial')"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs',penalty='l2', C=0.5)\n",
    "model.fit(X_train_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9347014925373134\n",
      "F-1 Score: [0.69565217 0.91176471 0.99310345 0.97658863 0.92640693 0.92436975\n",
      " 1.         0.89719626 0.96195652 0.83076923 0.9625     0.96202532\n",
      " 0.90277778]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F-1 Score:\", metrics.f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=300, random_state=1, solver='sgd')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(solver='sgd',random_state=1, max_iter=300)\n",
    "mlp.fit(X_train_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9440298507462687\n",
      "F-1 Score: [0.7761194  0.96969697 1.         0.95765472 0.93913043 0.94560669\n",
      " 0.98412698 0.91428571 0.95978552 0.89552239 0.95597484 0.96296296\n",
      " 0.91304348]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F-1 Score:\", metrics.f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1, max_iter=300, random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(solver='lbfgs',alpha = 1, random_state=1, max_iter=300)\n",
    "mlp.fit(X_train_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9281716417910447\n",
      "F-1 Score: [0.72       0.88888889 0.99300699 0.96052632 0.93449782 0.92244898\n",
      " 0.96774194 0.90640394 0.94623656 0.85294118 0.93506494 0.98734177\n",
      " 0.88405797]\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp.predict(X_test_std)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F-1 Score:\", metrics.f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURRENT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=10, max_iter=300, random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CURRENT MODEL\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(solver='lbfgs',alpha = 10, random_state=1, max_iter=300)\n",
    "mlp.fit(X_train_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'metadata_model.sav'\n",
    "pickle.dump(mlp, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9242105263157895\n",
      "F-1 Score: [0.74074074 0.97142857 1.         0.96478873 0.87553648 0.87782805\n",
      " 0.95454545 0.86384977 0.97222222 0.91304348 0.94       0.95454545\n",
      " 0.92561983]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F-1 Score:\", metrics.f1_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         bib_info       0.77      0.71      0.74        28\n",
      "   correspondence       0.97      0.97      0.97        35\n",
      "            dates       1.00      1.00      1.00        55\n",
      "    main_abstract       0.94      0.99      0.96       139\n",
      " main_affiliation       0.89      0.86      0.88       119\n",
      "     main_authors       0.84      0.92      0.88       106\n",
      "    main_keywords       1.00      0.91      0.95        23\n",
      "       main_title       0.90      0.83      0.86       111\n",
      "   other_abstract       0.98      0.97      0.97       181\n",
      "other_affiliation       0.91      0.91      0.91        23\n",
      "    other_authors       0.92      0.96      0.94        49\n",
      "   other_keywords       1.00      0.91      0.95        23\n",
      "      other_title       0.89      0.97      0.93        58\n",
      "\n",
      "         accuracy                           0.92       950\n",
      "        macro avg       0.93      0.92      0.92       950\n",
      "     weighted avg       0.92      0.92      0.92       950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/imblearn/utils/_validation.py:587: FutureWarning: Pass sampling_strategy=not majority as keyword args. From version 0.9 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "strtfdKFold = StratifiedKFold(n_splits=10)\n",
    "kfold = strtfdKFold.split(all_features, all_labels)\n",
    "scores = []\n",
    "# clf = SVC()\n",
    "over = SMOTE('not majority')\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs',penalty='l2', C=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         bib_info       0.69      0.82      0.75        11\n",
      "   correspondence       0.86      1.00      0.92        12\n",
      "            dates       0.97      1.00      0.98        28\n",
      "    main_abstract       0.97      0.94      0.95        62\n",
      " main_affiliation       0.95      0.93      0.94        56\n",
      "     main_authors       0.91      0.96      0.93        52\n",
      "    main_keywords       1.00      1.00      1.00        17\n",
      "       main_title       0.91      0.83      0.87        48\n",
      "   other_abstract       0.98      0.98      0.98        66\n",
      "other_affiliation       0.93      1.00      0.96        13\n",
      "    other_authors       0.95      0.91      0.93        23\n",
      "   other_keywords       1.00      1.00      1.00        15\n",
      "      other_title       1.00      0.96      0.98        26\n",
      "\n",
      "         accuracy                           0.94       429\n",
      "        macro avg       0.93      0.95      0.94       429\n",
      "     weighted avg       0.95      0.94      0.94       429\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         bib_info       0.35      0.89      0.50         9\n",
      "   correspondence       1.00      1.00      1.00        14\n",
      "            dates       1.00      0.96      0.98        26\n",
      "    main_abstract       0.96      0.94      0.95        72\n",
      " main_affiliation       0.91      0.89      0.90        47\n",
      "     main_authors       0.94      0.96      0.95        53\n",
      "    main_keywords       0.94      1.00      0.97        15\n",
      "       main_title       0.90      0.73      0.81        52\n",
      "   other_abstract       0.96      0.93      0.95        76\n",
      "other_affiliation       0.91      0.91      0.91        11\n",
      "    other_authors       0.95      0.90      0.92        20\n",
      "   other_keywords       0.91      0.83      0.87        12\n",
      "      other_title       0.87      0.91      0.89        22\n",
      "\n",
      "         accuracy                           0.91       429\n",
      "        macro avg       0.89      0.91      0.89       429\n",
      "     weighted avg       0.93      0.91      0.91       429\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         bib_info       0.62      0.62      0.62         8\n",
      "   correspondence       1.00      0.78      0.88         9\n",
      "            dates       1.00      0.94      0.97        18\n",
      "    main_abstract       0.91      0.97      0.94        61\n",
      " main_affiliation       0.94      0.88      0.91        51\n",
      "     main_authors       0.88      0.91      0.89        55\n",
      "    main_keywords       1.00      1.00      1.00        18\n",
      "       main_title       0.85      0.82      0.83        49\n",
      "   other_abstract       0.87      0.99      0.92        74\n",
      "other_affiliation       1.00      0.83      0.91        18\n",
      "    other_authors       0.97      0.94      0.95        31\n",
      "   other_keywords       0.93      0.88      0.90        16\n",
      "      other_title       0.94      0.81      0.87        21\n",
      "\n",
      "         accuracy                           0.91       429\n",
      "        macro avg       0.92      0.87      0.89       429\n",
      "     weighted avg       0.91      0.91      0.91       429\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         bib_info       0.90      0.53      0.67        17\n",
      "   correspondence       0.86      1.00      0.92        12\n",
      "            dates       1.00      1.00      1.00        29\n",
      "    main_abstract       1.00      0.96      0.98        54\n",
      " main_affiliation       0.97      0.91      0.94        65\n",
      "     main_authors       0.88      0.96      0.92        48\n",
      "    main_keywords       1.00      1.00      1.00        15\n",
      "       main_title       0.84      0.91      0.88        47\n",
      "   other_abstract       0.97      0.97      0.97        68\n",
      "other_affiliation       0.91      1.00      0.95        10\n",
      "    other_authors       0.96      1.00      0.98        26\n",
      "   other_keywords       0.92      1.00      0.96        11\n",
      "      other_title       0.93      0.93      0.93        27\n",
      "\n",
      "         accuracy                           0.94       429\n",
      "        macro avg       0.93      0.94      0.93       429\n",
      "     weighted avg       0.94      0.94      0.94       429\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         bib_info       0.70      0.70      0.70        10\n",
      "   correspondence       1.00      1.00      1.00        17\n",
      "            dates       0.97      1.00      0.98        29\n",
      "    main_abstract       0.97      0.97      0.97        61\n",
      " main_affiliation       0.96      0.98      0.97        44\n",
      "     main_authors       0.91      0.92      0.91        52\n",
      "    main_keywords       1.00      0.96      0.98        23\n",
      "       main_title       0.86      0.86      0.86        43\n",
      "   other_abstract       1.00      0.96      0.98        74\n",
      "other_affiliation       0.73      0.89      0.80         9\n",
      "    other_authors       0.92      0.96      0.94        25\n",
      "   other_keywords       1.00      1.00      1.00        11\n",
      "      other_title       0.93      0.87      0.90        31\n",
      "\n",
      "         accuracy                           0.94       429\n",
      "        macro avg       0.92      0.93      0.92       429\n",
      "     weighted avg       0.94      0.94      0.94       429\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         bib_info       0.90      0.90      0.90        10\n",
      "   correspondence       1.00      0.90      0.95        10\n",
      "            dates       0.98      0.98      0.98        41\n",
      "    main_abstract       1.00      0.97      0.99        72\n",
      " main_affiliation       0.96      0.95      0.95        55\n",
      "     main_authors       0.86      0.97      0.92        39\n",
      "    main_keywords       0.92      0.92      0.92        13\n",
      "       main_title       0.86      0.89      0.88        36\n",
      "   other_abstract       0.97      1.00      0.99        76\n",
      "other_affiliation       1.00      0.89      0.94        19\n",
      "    other_authors       0.86      1.00      0.93        19\n",
      "   other_keywords       1.00      0.94      0.97        18\n",
      "      other_title       1.00      0.80      0.89        20\n",
      "\n",
      "         accuracy                           0.95       428\n",
      "        macro avg       0.95      0.93      0.94       428\n",
      "     weighted avg       0.95      0.95      0.95       428\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         bib_info       0.71      0.60      0.65        20\n",
      "   correspondence       1.00      1.00      1.00        13\n",
      "            dates       1.00      1.00      1.00        26\n",
      "    main_abstract       0.97      0.95      0.96        60\n",
      " main_affiliation       0.96      0.95      0.96        58\n",
      "     main_authors       0.93      0.95      0.94        58\n",
      "    main_keywords       1.00      1.00      1.00        13\n",
      "       main_title       0.84      0.86      0.85        37\n",
      "   other_abstract       0.96      0.98      0.97        65\n",
      "other_affiliation       0.94      0.94      0.94        17\n",
      "    other_authors       0.87      0.91      0.89        22\n",
      "   other_keywords       0.94      0.94      0.94        16\n",
      "      other_title       0.91      0.91      0.91        23\n",
      "\n",
      "         accuracy                           0.93       428\n",
      "        macro avg       0.93      0.92      0.92       428\n",
      "     weighted avg       0.93      0.93      0.93       428\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "         bib_info       0.99      0.89      0.94       222\n",
      "   correspondence       1.00      0.60      0.75         5\n",
      "            dates       1.00      0.92      0.96        12\n",
      "    main_abstract       0.95      0.98      0.97        43\n",
      " main_affiliation       0.86      0.89      0.87        27\n",
      "     main_authors       0.82      0.95      0.88        19\n",
      "    main_keywords       1.00      1.00      1.00         8\n",
      "       main_title       0.50      0.88      0.64        17\n",
      "   other_abstract       0.93      1.00      0.96        37\n",
      "other_affiliation       1.00      1.00      1.00         3\n",
      "    other_authors       1.00      1.00      1.00        20\n",
      "   other_keywords       1.00      0.80      0.89         5\n",
      "      other_title       0.62      1.00      0.77        10\n",
      "\n",
      "         accuracy                           0.92       428\n",
      "        macro avg       0.90      0.92      0.89       428\n",
      "     weighted avg       0.94      0.92      0.92       428\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "        bib_info       1.00      0.89      0.94       222\n",
      "  correspondence       1.00      1.00      1.00       204\n",
      "main_affiliation       1.00      1.00      1.00         1\n",
      "    main_authors       1.00      1.00      1.00         1\n",
      "      main_title       0.00      0.00      0.00         0\n",
      "     other_title       0.00      0.00      0.00         0\n",
      "\n",
      "        accuracy                           0.94       428\n",
      "       macro avg       0.67      0.65      0.66       428\n",
      "    weighted avg       1.00      0.94      0.97       428\n",
      "\n",
      "9\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      bib_info       0.70      1.00      0.82         7\n",
      "correspondence       1.00      1.00      1.00       240\n",
      "         dates       1.00      0.98      0.99       181\n",
      "    main_title       0.00      0.00      0.00         0\n",
      "\n",
      "      accuracy                           0.99       428\n",
      "     macro avg       0.67      0.74      0.70       428\n",
      "  weighted avg       0.99      0.99      0.99       428\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/yasmeen/anaconda3/envs/auto_peer/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for k, (train, test) in enumerate(kfold):\n",
    "    X_k_train = X.iloc[train, :]\n",
    "    y_k_train = y.iloc[train]\n",
    "    X_K_test = X.iloc[test,:]\n",
    "    y_K_test = y.iloc[test]\n",
    "    \n",
    "    X_k_train, y_k_train = over.fit_resample(X_k_train, y_k_train)\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_k_train)\n",
    "    X_train_std = scaler.transform(X_k_train)\n",
    "    X_test_std = scaler.transform(X_K_test)\n",
    "    \n",
    "#     clf.fit(X_train_std, y_k_train)\n",
    "    model.fit(X_train_std, y_k_train)\n",
    "    y_pred = model.predict(X_test_std)\n",
    "    scores.append(metrics.accuracy_score(y_K_test, y_pred))\n",
    "    \n",
    "    scores.append(metrics.f1_score(y_K_test, y_pred, average=None))\n",
    "    \n",
    "    print(k)\n",
    "    print(metrics.classification_report(y_K_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.936533015271333"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(scores[0::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.936533015271333"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores[0::2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
